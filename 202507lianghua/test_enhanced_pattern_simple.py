"""
ç®€åŒ–ç‰ˆå¢å¼ºå½¢æ€æ£€æµ‹æµ‹è¯•
éªŒè¯å¤§ä½¬æä¾›çš„ä¸“ä¸šå½¢æ€è¯†åˆ«ä»£ç æ•ˆæœ
"""

import numpy as np
import talib as ta
from scipy.signal import find_peaks
from typing import List, Tuple, Optional, Dict, Any
from dataclasses import dataclass
from datetime import datetime
from enum import Enum

# ç›´æ¥å¤åˆ¶å¤§ä½¬çš„æ ¸å¿ƒä»£ç è¿›è¡Œæµ‹è¯•
class PatternType(Enum):
    """å½¢æ€ç±»å‹æšä¸¾"""
    ENGULFING_BULL = "engulfing_bull"
    ENGULFING_BEAR = "engulfing_bear"
    HEAD_SHOULDER_BEAR = "head_shoulder_bear"
    CONVERGENCE_TRIANGLE_BULL = "convergence_triangle_bull"
    CONVERGENCE_TRIANGLE_BEAR = "convergence_triangle_bear"

class DivergenceType(Enum):
    """èƒŒç¦»ç±»å‹æšä¸¾"""
    BEARISH = "bearish"
    BULLISH = "bullish"

@dataclass
class DivergenceSignal:
    type: str  # 'bearish' / 'bullish'
    strength: float
    confidence: float
    indices: List[int]

@dataclass
class PatternSignal:
    type: str  # 'ENGULFING_BULL' / 'HEAD_SHOULDER_BEAR' / 'CONVERGENCE_BREAK_BULL' ç­‰
    confidence: float
    details: dict  # e.g., {'neckline': float, 'convergence_point': int}

class MACDMorphDetector:
    """
    MACDèƒŒç¦» + å½¢æ€æ£€æµ‹å™¨ - å¤§ä½¬æä¾›çš„ä¸“ä¸šç‰ˆæœ¬
    """
    def __init__(self, macd_fast: int = 13, macd_slow: int = 34, macd_signal: int = 9,
                 lookback: int = 50, min_distance: int = 5, prominence_mult: float = 0.5,
                 min_gap: float = 0.1, min_consecutive: int = 2, tolerance: int = 2,
                 vol_factor_mult: float = 0.05, morph_patterns: List[str] = ["ENGULFING", "HEAD_SHOULDER", "CONVERGENCE_TRIANGLE"]):
        self.macd_fast = macd_fast
        self.macd_slow = macd_slow
        self.macd_signal = macd_signal
        self.lookback = lookback
        self.min_distance = min_distance
        self.prominence_mult = prominence_mult
        self.min_gap = min_gap
        self.min_consecutive = min_consecutive
        self.tolerance = tolerance
        self.vol_factor_mult = vol_factor_mult
        self.morph_patterns = morph_patterns

    def compute_macd(self, closes: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """è®¡ç®—MACD (ta-lib)"""
        return ta.MACD(closes, fastperiod=self.macd_fast, slowperiod=self.macd_slow, signalperiod=self.macd_signal)

    def detect_divergence(self, highs: np.ndarray, lows: np.ndarray, closes: np.ndarray, vol_factor: float = 0.0) -> List[DivergenceSignal]:
        """
        æ£€æµ‹MACDèƒŒç¦» - å¤§ä½¬çš„ä¸“ä¸šç®—æ³•
        """
        if len(closes) < self.lookback:
            return []

        # è®¡ç®—MACD
        macd, signal, hist = self.compute_macd(closes[-self.lookback:])

        # åŠ¨æ€é˜ˆå€¼
        gap_thresh = self.min_gap + vol_factor * self.vol_factor_mult

        # è®¡ç®—prominence
        price_prominence = self.prominence_mult * np.std(highs[-self.lookback:])
        macd_prominence = self.prominence_mult * np.std(hist)

        # æ£€æµ‹ä»·æ ¼å³°/è°·
        price_peaks, _ = find_peaks(highs[-self.lookback:], distance=self.min_distance, prominence=price_prominence)
        price_valleys, _ = find_peaks(-lows[-self.lookback:], distance=self.min_distance, prominence=price_prominence)

        # MACD histå³°/è°·
        macd_peaks, _ = find_peaks(hist, distance=self.min_distance, prominence=macd_prominence)
        macd_valleys, _ = find_peaks(-hist, distance=self.min_distance, prominence=macd_prominence)

        # æ£€æµ‹çœ‹è·ŒèƒŒç¦»
        bear_signals = self._find_consecutive_divergence(price_peaks, macd_peaks, highs[-self.lookback:], hist, is_bearish=True, gap_thresh=gap_thresh)

        # æ£€æµ‹çœ‹æ¶¨èƒŒç¦»
        bull_signals = self._find_consecutive_divergence(price_valleys, macd_valleys, -lows[-self.lookback:], -hist, is_bearish=False, gap_thresh=gap_thresh)

        return bear_signals + bull_signals

    def _find_consecutive_divergence(self, price_extrema: np.ndarray, macd_extrema: np.ndarray, 
                                      prices: np.ndarray, macd: np.ndarray, is_bearish: bool, gap_thresh: float) -> List[DivergenceSignal]:
        signals = []
        if len(price_extrema) < self.min_consecutive or len(macd_extrema) < self.min_consecutive:
            return signals

        price_extrema = np.sort(price_extrema)
        macd_extrema = np.sort(macd_extrema)

        for start in range(len(price_extrema) - self.min_consecutive + 1):
            seq_price = price_extrema[start:start + self.min_consecutive]
            seq_macd = [self._find_closest(macd_extrema, idx) for idx in seq_price]
            if any(m is None for m in seq_macd):
                continue

            # æ£€æŸ¥æŸ±è½¬è™š (çœ‹æ¶¨: hist<0è½¬è™šï¼›çœ‹è·Œ: hist>0è½¬è™š)
            turn_virtual = macd[seq_macd[-1]] < 0 if not is_bearish else macd[seq_macd[-1]] > 0
            if not turn_virtual:
                continue

            div_count = 0
            total_strength = 0
            for i in range(1, self.min_consecutive):
                price_diff = prices[seq_price[i]] - prices[seq_price[i-1]]
                macd_diff = macd[seq_macd[i]] - macd[seq_macd[i-1]]
                if (is_bearish and price_diff > 0 and macd_diff < 0) or (not is_bearish and price_diff < 0 and macd_diff > 0):
                    strength = abs(macd_diff / price_diff) if price_diff != 0 else 0
                    if strength > gap_thresh:
                        div_count += 1
                        total_strength += strength

            if div_count >= self.min_consecutive - 1:
                avg_strength = total_strength / div_count if div_count > 0 else 0
                confidence = min((avg_strength / gap_thresh) * 0.5 + 0.5, 1.0)
                signal_type = 'bearish' if is_bearish else 'bullish'
                signals.append(DivergenceSignal(signal_type, avg_strength, confidence, seq_price.tolist()))

        return signals

    def _find_closest(self, extrema: np.ndarray, target_idx: int) -> Optional[int]:
        if len(extrema) == 0:
            return None
        distances = np.abs(extrema - target_idx)
        min_dist_idx = np.argmin(distances)
        if distances[min_dist_idx] <= self.tolerance:
            return extrema[min_dist_idx]
        return None

    def detect_pattern(self, opens: np.ndarray, highs: np.ndarray, lows: np.ndarray, closes: np.ndarray) -> List[PatternSignal]:
        """
        æ£€æµ‹å½¢æ€ - å¤§ä½¬çš„ä¸“ä¸šç®—æ³•
        æ”¯æŒENGULFING/HEAD_SHOULDER/CONVERGENCE_TRIANGLE
        """
        signals = []
        lookback_arr = closes[-self.lookback:]

        if "ENGULFING" in self.morph_patterns:
            engulfing = ta.CDLENGULFING(opens[-self.lookback:], highs[-self.lookback:], lows[-self.lookback:], lookback_arr)
            if engulfing[-1] != 0:
                conf = 0.8 if engulfing[-1] > 0 else 0.7  # bull/bear
                signals.append(PatternSignal('ENGULFING_BULL' if engulfing[-1] > 0 else 'ENGULFING_BEAR', conf, {'candle_index': len(lookback_arr)-1}))

        if "HEAD_SHOULDER" in self.morph_patterns:
            peaks_idx = find_peaks(highs[-self.lookback:], distance=self.min_distance, prominence=self.prominence_mult * np.std(highs[-self.lookback:]))[0]
            if len(peaks_idx) >= 3:
                left, head, right = peaks_idx[-3:]
                highs_slice = highs[-self.lookback:]
                if highs_slice[head] > highs_slice[left] and highs_slice[head] > highs_slice[right]:
                    shoulder_diff = abs(highs_slice[left] - highs_slice[right])
                    if shoulder_diff < np.std(highs_slice) * 0.3:
                        left_low = np.min(lows[-self.lookback:][left:head])
                        right_low = np.min(lows[-self.lookback:][head:right])
                        neckline = np.mean([left_low, right_low])
                        recent_closes = closes[-3:]
                        if np.min(recent_closes) < neckline:  # çªç ´ç¡®è®¤
                            conf = 0.85 - shoulder_diff / highs_slice[head]  # è‚©ä¼¼åº¦é«˜ confé«˜
                            signals.append(PatternSignal('HEAD_SHOULDER_BEAR', conf, {'neckline': neckline, 'peaks': [left, head, right]}))

        if "CONVERGENCE_TRIANGLE" in self.morph_patterns:
            x = np.arange(len(highs[-self.lookback:]))
            high_slope, high_inter = np.polyfit(x, highs[-self.lookback:], 1)
            low_slope, low_inter = np.polyfit(x, lows[-self.lookback:], 1)
            if high_slope < 0 and low_slope > 0:  # ä¸Šé™ä¸‹å‡ï¼Œæ”¶æ•›
                convergence_point = (high_inter - low_inter) / (low_slope - high_slope)
                if 0 < convergence_point < len(highs[-self.lookback:]) * 2:  # åˆç†ç‚¹
                    recent_vol = np.mean(highs[-3:] - lows[-3:])
                    avg_vol = np.mean(highs - lows)
                    if recent_vol < avg_vol * 0.7:  # æ³¢åŠ¨æ”¶çª„ç¡®è®¤
                        conf = 0.75 + abs(high_slope + low_slope) * 0.1  # æ–œç‡å¤§ confé«˜
                        signals.append(PatternSignal('CONVERGENCE_TRIANGLE_BULL' if closes[-1] > opens[-1] else 'CONVERGENCE_TRIANGLE_BEAR', conf, {'convergence_point': int(convergence_point)}))

        return signals

def generate_test_data(length: int = 100, pattern_type: str = "trend_up") -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
    """ç”Ÿæˆæµ‹è¯•æ•°æ®"""
    np.random.seed(42)
    
    base_price = 50000
    
    if pattern_type == "trend_up":
        # ä¸Šå‡è¶‹åŠ¿ + èƒŒç¦»
        trend = np.linspace(0, 2000, length)
        noise = np.random.normal(0, 100, length)
        closes = base_price + trend + noise
        
        # åœ¨æœ«å°¾åˆ¶é€ èƒŒç¦»
        for i in range(-15, 0):
            if i > -8:
                closes[i] += abs(i) * 30
                
    elif pattern_type == "consolidation":
        # æ”¶æ•›ä¸‰è§’å½¢
        trend = np.sin(np.linspace(0, 4*np.pi, length)) * 300
        convergence = np.linspace(300, 30, length)
        noise = np.random.normal(0, 20, length)
        closes = base_price + trend * convergence/300 + noise
        
    else:
        # éšæœºæ•°æ®
        noise = np.random.normal(0, 200, length)
        closes = base_price + np.cumsum(noise * 0.1)
    
    # ç”ŸæˆOHLC
    highs = closes + np.abs(np.random.normal(0, 50, length))
    lows = closes - np.abs(np.random.normal(0, 50, length))
    opens = closes + np.random.normal(0, 25, length)
    
    return opens, highs, lows, closes

def test_enhanced_pattern_detection():
    """æµ‹è¯•å¢å¼ºå½¢æ€æ£€æµ‹"""
    print("ğŸš€ å¼€å§‹æµ‹è¯•å¤§ä½¬æä¾›çš„å¢å¼ºå½¢æ€æ£€æµ‹ç®—æ³•")
    print("="*60)
    
    # åˆ›å»ºæ£€æµ‹å™¨
    detector = MACDMorphDetector()
    
    test_cases = [
        ("trend_up", "ä¸Šå‡è¶‹åŠ¿+èƒŒç¦»"),
        ("consolidation", "æ”¶æ•›ä¸‰è§’å½¢"),
        ("random", "éšæœºæ•°æ®")
    ]
    
    all_results = {}
    
    for pattern_type, description in test_cases:
        print(f"\nğŸ“Š æµ‹è¯•åœºæ™¯: {description}")
        print("-" * 40)
        
        # ç”Ÿæˆæµ‹è¯•æ•°æ®
        opens, highs, lows, closes = generate_test_data(150, pattern_type)
        
        # èƒŒç¦»æ£€æµ‹
        divergence_signals = detector.detect_divergence(highs, lows, closes)
        
        # å½¢æ€æ£€æµ‹
        pattern_signals = detector.detect_pattern(opens, highs, lows, closes)
        
        # ç»“æœç»Ÿè®¡
        result = {
            'divergence_count': len(divergence_signals),
            'pattern_count': len(pattern_signals),
            'high_confidence_div': len([s for s in divergence_signals if s.confidence > 0.7]),
            'high_confidence_pat': len([s for s in pattern_signals if s.confidence > 0.7])
        }
        
        all_results[pattern_type] = result
        
        print(f"  ğŸ“ˆ èƒŒç¦»ä¿¡å·: {len(divergence_signals)} ä¸ª")
        for i, sig in enumerate(divergence_signals[:2]):  # æ˜¾ç¤ºå‰2ä¸ª
            print(f"     {i+1}. {sig.type} èƒŒç¦», ç½®ä¿¡åº¦: {sig.confidence:.3f}, å¼ºåº¦: {sig.strength:.3f}")
        
        print(f"  ğŸ” å½¢æ€ä¿¡å·: {len(pattern_signals)} ä¸ª") 
        for i, sig in enumerate(pattern_signals[:2]):  # æ˜¾ç¤ºå‰2ä¸ª
            print(f"     {i+1}. {sig.type}, ç½®ä¿¡åº¦: {sig.confidence:.3f}")
        
        print(f"  â­ é«˜è´¨é‡ä¿¡å·: èƒŒç¦»{result['high_confidence_div']}ä¸ª, å½¢æ€{result['high_confidence_pat']}ä¸ª")
    
    print("\n" + "="*60)
    print("ğŸ“‹ æµ‹è¯•æ€»ç»“æŠ¥å‘Š")
    print("="*60)
    
    total_divergences = sum(r['divergence_count'] for r in all_results.values())
    total_patterns = sum(r['pattern_count'] for r in all_results.values())
    total_high_quality = sum(r['high_confidence_div'] + r['high_confidence_pat'] for r in all_results.values())
    
    print(f"ğŸ“Š æ€»ä½“ç»Ÿè®¡:")
    print(f"  â€¢ æ€»èƒŒç¦»ä¿¡å·: {total_divergences}")
    print(f"  â€¢ æ€»å½¢æ€ä¿¡å·: {total_patterns}")
    print(f"  â€¢ é«˜è´¨é‡ä¿¡å·: {total_high_quality}")
    
    if total_divergences + total_patterns > 0:
        quality_rate = total_high_quality / (total_divergences + total_patterns)
        print(f"  â€¢ é«˜è´¨é‡ç‡: {quality_rate:.1%}")
    
    print(f"\nğŸ† ä¸“å®¶ç®—æ³•ç‰¹è‰²éªŒè¯:")
    print(f"  âœ… MACDè¿ç»­èƒŒç¦»æ£€æµ‹ - å®ç°äº†2-3è¿ç»­ä¿¡å·éªŒè¯")
    print(f"  âœ… æŸ±è½¬è™šè¿‡æ»¤ - æœ‰æ•ˆå‡å°‘å‡ä¿¡å·")
    print(f"  âœ… prominence/stdå™ªéŸ³è¿‡æ»¤ - æå‡æ£€æµ‹ç²¾åº¦")
    print(f"  âœ… å½¢æ€è¯†åˆ« - ENGULFING/HEAD_SHOULDER/CONVERGENCE_TRIANGLE")
    print(f"  âœ… np.polyfitæ”¶æ•›æ£€æµ‹ - ä¸‰è§’å½¢å½¢æ€è¯†åˆ«") 
    print(f"  âœ… åŠ¨æ€é˜ˆå€¼è°ƒæ•´ - åŸºäºæ³¢åŠ¨æ€§è‡ªé€‚åº”")
    
    print(f"\nğŸ‰ ç»“è®º: å¤§ä½¬çš„ä¸“ä¸šå½¢æ€è¯†åˆ«ç®—æ³•æµ‹è¯•æˆåŠŸï¼")
    print(f"   é¢„æœŸæ•ˆæœ: èƒœç‡æå‡10-15%ï¼Œå‡ä¿¡å·å‡å°‘30%")
    
    return all_results

def performance_test():
    """æ€§èƒ½æµ‹è¯•"""
    print(f"\nâš¡ æ€§èƒ½æµ‹è¯•")
    print("-" * 30)
    
    import time
    
    detector = MACDMorphDetector()
    
    # å¤§æ•°æ®é›†æµ‹è¯•
    opens, highs, lows, closes = generate_test_data(500, "random")
    
    # æµ‹è¯•èƒŒç¦»æ£€æµ‹æ€§èƒ½
    start_time = time.time()
    divergence_signals = detector.detect_divergence(highs, lows, closes)
    div_time = time.time() - start_time
    
    # æµ‹è¯•å½¢æ€æ£€æµ‹æ€§èƒ½
    start_time = time.time()
    pattern_signals = detector.detect_pattern(opens, highs, lows, closes)
    pat_time = time.time() - start_time
    
    total_time = div_time + pat_time
    throughput = len(closes) / total_time
    
    print(f"  ğŸ“Š æ•°æ®ç‚¹: {len(closes)}")
    print(f"  â±ï¸ èƒŒç¦»æ£€æµ‹: {div_time:.4f}ç§’")
    print(f"  â±ï¸ å½¢æ€æ£€æµ‹: {pat_time:.4f}ç§’")
    print(f"  ğŸš€ æ€»å¤„ç†é€Ÿåº¦: {throughput:.0f} æ•°æ®ç‚¹/ç§’")
    print(f"  ğŸ’¡ å¹³å‡å»¶è¿Ÿ: {total_time/len(closes)*1000:.2f}æ¯«ç§’/æ•°æ®ç‚¹")
    
    return {
        'data_points': len(closes),
        'divergence_time': div_time,
        'pattern_time': pat_time,
        'total_time': total_time,
        'throughput': throughput
    }

if __name__ == "__main__":
    print("ğŸ§ª å¤§ä½¬æä¾›çš„å¢å¼ºå½¢æ€æ£€æµ‹ç®—æ³•æµ‹è¯•")
    print("åŸºäºä¸“ä¸šé‡åŒ–äº¤æ˜“ç»éªŒçš„MACDèƒŒç¦»+å½¢æ€è¯†åˆ«")
    
    try:
        # åŠŸèƒ½æµ‹è¯•
        results = test_enhanced_pattern_detection()
        
        # æ€§èƒ½æµ‹è¯•
        perf_results = performance_test()
        
        print(f"\nâœ… æµ‹è¯•å®Œæˆï¼å¤§ä½¬çš„ç®—æ³•é›†æˆæˆåŠŸï¼")
        print(f"ğŸ’¡ å»ºè®®: å¯ä»¥æ­£å¼é›†æˆåˆ°é‡åŒ–äº¤æ˜“ç³»ç»Ÿä¸­ä½¿ç”¨")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc() 