#!/usr/bin/env python3
"""
Rediså®¢æˆ·ç«¯æµ‹è¯•
"""

import sys
import os
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import time
import asyncio
import threading
from datetime import datetime
from config.config_manager import ConfigManager
from utils.redis_client import RedisClient, CacheLevel, LockType

def test_redis_client():
    """æµ‹è¯•Rediså®¢æˆ·ç«¯"""
    print("ğŸ§ª æµ‹è¯•Rediså®¢æˆ·ç«¯æ¨¡å—")
    print("=" * 60)
    
    try:
        # 1. åˆå§‹åŒ–é…ç½®
        config = ConfigManager()
        print("âœ… é…ç½®ç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
        
        # 2. åˆå§‹åŒ–Rediså®¢æˆ·ç«¯
        redis_client = RedisClient(config)
        print("âœ… Rediså®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
        
        # 3. æµ‹è¯•åŸºæœ¬ç¼“å­˜æ“ä½œ
        print("\nğŸ“Š æµ‹è¯•åŸºæœ¬ç¼“å­˜æ“ä½œï¼š")
        
        # è®¾ç½®ç¼“å­˜
        success = redis_client.set("test_key", "test_value", ttl=60)
        print(f"âœ… è®¾ç½®ç¼“å­˜: {success}")
        
        # è·å–ç¼“å­˜
        value = redis_client.get("test_key")
        print(f"âœ… è·å–ç¼“å­˜: {value}")
        
        # æ£€æŸ¥å­˜åœ¨
        exists = redis_client.exists("test_key")
        print(f"âœ… æ£€æŸ¥å­˜åœ¨: {exists}")
        
        # è·å–TTL
        ttl = redis_client.ttl("test_key")
        print(f"âœ… è·å–TTL: {ttl}ç§’")
        
        # åˆ é™¤ç¼“å­˜
        deleted = redis_client.delete("test_key")
        print(f"âœ… åˆ é™¤ç¼“å­˜: {deleted}")
        
        # å†æ¬¡è·å–ï¼ˆåº”è¯¥ä¸ºNoneï¼‰
        value_after_delete = redis_client.get("test_key")
        print(f"âœ… åˆ é™¤åè·å–: {value_after_delete}")
        
        # 4. æµ‹è¯•ä¸åŒç¼“å­˜çº§åˆ«
        print("\nğŸ“Š æµ‹è¯•ä¸åŒç¼“å­˜çº§åˆ«ï¼š")
        
        # å†…å­˜ç¼“å­˜
        redis_client.set("memory_key", {"type": "memory", "value": 123}, 
                        ttl=30, level=CacheLevel.L1_MEMORY)
        memory_value = redis_client.get("memory_key", level=CacheLevel.L1_MEMORY)
        print(f"âœ… å†…å­˜ç¼“å­˜: {memory_value}")
        
        # Redisç¼“å­˜
        redis_client.set("redis_key", {"type": "redis", "value": 456}, 
                        ttl=30, level=CacheLevel.L2_REDIS)
        redis_value = redis_client.get("redis_key", level=CacheLevel.L2_REDIS)
        print(f"âœ… Redisç¼“å­˜: {redis_value}")
        
        # ç£ç›˜ç¼“å­˜ï¼ˆå ä½å®ç°ï¼‰
        redis_client.set("disk_key", {"type": "disk", "value": 789}, 
                        ttl=30, level=CacheLevel.L3_DISK)
        disk_value = redis_client.get("disk_key", level=CacheLevel.L3_DISK)
        print(f"âœ… ç£ç›˜ç¼“å­˜: {disk_value}")
        
        # 5. æµ‹è¯•å“ˆå¸Œæ“ä½œ
        print("\nğŸ“Š æµ‹è¯•å“ˆå¸Œæ“ä½œï¼š")
        
        # è®¾ç½®å“ˆå¸Œå­—æ®µ
        hash_set = redis_client.set_hash("user:1001", "name", "å¼ ä¸‰", ttl=60)
        print(f"âœ… è®¾ç½®å“ˆå¸Œå­—æ®µname: {hash_set}")
        
        hash_set = redis_client.set_hash("user:1001", "age", 25, ttl=60)
        print(f"âœ… è®¾ç½®å“ˆå¸Œå­—æ®µage: {hash_set}")
        
        hash_set = redis_client.set_hash("user:1001", "email", "zhangsan@example.com", ttl=60)
        print(f"âœ… è®¾ç½®å“ˆå¸Œå­—æ®µemail: {hash_set}")
        
        # è·å–å“ˆå¸Œå­—æ®µ
        name = redis_client.get_hash("user:1001", "name")
        age = redis_client.get_hash("user:1001", "age")
        email = redis_client.get_hash("user:1001", "email")
        
        print(f"âœ… è·å–å“ˆå¸Œå­—æ®µ:")
        print(f"   name: {name}")
        print(f"   age: {age}")
        print(f"   email: {email}")
        
        # åˆ é™¤å“ˆå¸Œå­—æ®µ
        deleted_field = redis_client.delete_hash("user:1001", "email")
        print(f"âœ… åˆ é™¤å“ˆå¸Œå­—æ®µemail: {deleted_field}")
        
        # å†æ¬¡è·å–è¢«åˆ é™¤çš„å­—æ®µ
        email_after_delete = redis_client.get_hash("user:1001", "email")
        print(f"âœ… åˆ é™¤åè·å–email: {email_after_delete}")
        
        # 6. æµ‹è¯•é›†åˆæ“ä½œ
        print("\nğŸ“Š æµ‹è¯•é›†åˆæ“ä½œï¼š")
        
        # æ·»åŠ åˆ°é›†åˆ
        added_count = redis_client.add_to_set("tags", "python", "redis", "cache", ttl=60)
        print(f"âœ… æ·»åŠ åˆ°é›†åˆ: {added_count} ä¸ªå…ƒç´ ")
        
        # å†æ¬¡æ·»åŠ ç›¸åŒå…ƒç´ 
        added_count = redis_client.add_to_set("tags", "python", "database")
        print(f"âœ… å†æ¬¡æ·»åŠ åˆ°é›†åˆ: {added_count} ä¸ªæ–°å…ƒç´ ")
        
        # æ£€æŸ¥æˆå‘˜
        is_member = redis_client.is_member_of_set("tags", "python")
        print(f"âœ… æ£€æŸ¥pythonæ˜¯å¦åœ¨é›†åˆä¸­: {is_member}")
        
        is_member = redis_client.is_member_of_set("tags", "java")
        print(f"âœ… æ£€æŸ¥javaæ˜¯å¦åœ¨é›†åˆä¸­: {is_member}")
        
        # è·å–æ‰€æœ‰æˆå‘˜
        members = redis_client.get_set_members("tags")
        print(f"âœ… è·å–é›†åˆæ‰€æœ‰æˆå‘˜: {sorted(list(members))}")
        
        # 7. æµ‹è¯•è®¡æ•°å™¨æ“ä½œ
        print("\nğŸ“Š æµ‹è¯•è®¡æ•°å™¨æ“ä½œï¼š")
        
        # é€’å¢è®¡æ•°å™¨
        counter_value = redis_client.increment("page_views", 1, ttl=60)
        print(f"âœ… é€’å¢è®¡æ•°å™¨: {counter_value}")
        
        counter_value = redis_client.increment("page_views", 5)
        print(f"âœ… å†æ¬¡é€’å¢è®¡æ•°å™¨: {counter_value}")
        
        # è·å–è®¡æ•°å™¨å€¼
        current_count = redis_client.get_counter("page_views")
        print(f"âœ… è·å–è®¡æ•°å™¨å€¼: {current_count}")
        
        # é€’å¢ä¸å­˜åœ¨çš„è®¡æ•°å™¨
        new_counter = redis_client.increment("new_counter", 10, ttl=30)
        print(f"âœ… æ–°è®¡æ•°å™¨å€¼: {new_counter}")
        
        # 8. æµ‹è¯•åˆ†å¸ƒå¼é”
        print("\nğŸ“Š æµ‹è¯•åˆ†å¸ƒå¼é”ï¼š")
        
        # è·å–æ’ä»–é”
        lock_acquired = redis_client.acquire_lock("test_lock", timeout=30, lock_type=LockType.EXCLUSIVE)
        print(f"âœ… è·å–æ’ä»–é”: {lock_acquired}")
        
        # å°è¯•å†æ¬¡è·å–ç›¸åŒé”ï¼ˆåº”è¯¥å¤±è´¥ï¼‰
        lock_acquired_again = redis_client.acquire_lock("test_lock", timeout=30, lock_type=LockType.EXCLUSIVE)
        print(f"âœ… å†æ¬¡è·å–ç›¸åŒé”: {lock_acquired_again}")
        
        # é‡Šæ”¾é”
        lock_released = redis_client.release_lock("test_lock")
        print(f"âœ… é‡Šæ”¾é”: {lock_released}")
        
        # è·å–å…±äº«é”
        shared_lock1 = redis_client.acquire_lock("shared_lock", timeout=30, lock_type=LockType.SHARED)
        print(f"âœ… è·å–å…±äº«é”1: {shared_lock1}")
        
        shared_lock2 = redis_client.acquire_lock("shared_lock", timeout=30, lock_type=LockType.SHARED)
        print(f"âœ… è·å–å…±äº«é”2: {shared_lock2}")
        
        # é‡Šæ”¾å…±äº«é”
        redis_client.release_lock("shared_lock")
        print("âœ… é‡Šæ”¾å…±äº«é”")
        
        # 9. æµ‹è¯•é”ä¸Šä¸‹æ–‡ç®¡ç†å™¨
        print("\nğŸ“Š æµ‹è¯•é”ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼š")
        
        async def test_lock_context():
            try:
                async with redis_client.lock_context("context_lock", timeout=10):
                    print("âœ… åœ¨é”ä¸Šä¸‹æ–‡ä¸­æ‰§è¡Œæ“ä½œ")
                    await asyncio.sleep(0.1)
                    print("âœ… æ“ä½œå®Œæˆ")
                print("âœ… é”è‡ªåŠ¨é‡Šæ”¾")
                return True
            except Exception as e:
                print(f"âŒ é”ä¸Šä¸‹æ–‡æµ‹è¯•å¤±è´¥: {e}")
                return False
        
        # è¿è¡Œå¼‚æ­¥æµ‹è¯•
        context_result = asyncio.run(test_lock_context())
        
        # 10. æµ‹è¯•å¹¶å‘é”æ“ä½œ
        print("\nğŸ“Š æµ‹è¯•å¹¶å‘é”æ“ä½œï¼š")
        
        def worker_thread(worker_id, lock_name, results):
            try:
                acquired = redis_client.acquire_lock(f"{lock_name}_{worker_id}", timeout=5)
                if acquired:
                    time.sleep(0.1)  # æ¨¡æ‹Ÿå·¥ä½œ
                    redis_client.release_lock(f"{lock_name}_{worker_id}")
                    results[worker_id] = True
                else:
                    results[worker_id] = False
            except Exception as e:
                print(f"å·¥ä½œçº¿ç¨‹{worker_id}é”™è¯¯: {e}")
                results[worker_id] = False
        
        # å¯åŠ¨å¤šä¸ªçº¿ç¨‹
        results = {}
        threads = []
        
        for i in range(5):
            thread = threading.Thread(target=worker_thread, args=(i, "concurrent_lock", results))
            threads.append(thread)
            thread.start()
        
        # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆ
        for thread in threads:
            thread.join()
        
        successful_locks = sum(1 for success in results.values() if success)
        print(f"âœ… å¹¶å‘é”æµ‹è¯•: {successful_locks}/{len(results)} ä¸ªé”æˆåŠŸè·å–")
        
        # 11. æµ‹è¯•ç¼“å­˜è¿‡æœŸ
        print("\nğŸ“Š æµ‹è¯•ç¼“å­˜è¿‡æœŸï¼š")
        
        # è®¾ç½®çŸ­è¿‡æœŸæ—¶é—´çš„ç¼“å­˜
        redis_client.set("expire_test", "will_expire", ttl=1)
        print("âœ… è®¾ç½®1ç§’è¿‡æœŸçš„ç¼“å­˜")
        
        # ç«‹å³è·å–
        value_before = redis_client.get("expire_test")
        print(f"âœ… è¿‡æœŸå‰è·å–: {value_before}")
        
        # ç­‰å¾…è¿‡æœŸ
        time.sleep(1.5)
        
        # è¿‡æœŸåè·å–
        value_after = redis_client.get("expire_test")
        print(f"âœ… è¿‡æœŸåè·å–: {value_after}")
        
        # 12. æµ‹è¯•æ‰¹é‡æ“ä½œå’Œæ€§èƒ½
        print("\nğŸ“Š æµ‹è¯•æ‰¹é‡æ“ä½œå’Œæ€§èƒ½ï¼š")
        
        # æ‰¹é‡è®¾ç½®
        start_time = time.time()
        for i in range(100):
            redis_client.set(f"batch_key_{i}", f"batch_value_{i}", ttl=30)
        
        set_time = time.time() - start_time
        print(f"âœ… æ‰¹é‡è®¾ç½®100ä¸ªé”®: {set_time:.4f}ç§’")
        
        # æ‰¹é‡è·å–
        start_time = time.time()
        retrieved_count = 0
        for i in range(100):
            value = redis_client.get(f"batch_key_{i}")
            if value is not None:
                retrieved_count += 1
        
        get_time = time.time() - start_time
        print(f"âœ… æ‰¹é‡è·å–100ä¸ªé”®: {get_time:.4f}ç§’, æˆåŠŸè·å–: {retrieved_count}")
        
        # è®¡ç®—æ“ä½œé€Ÿåº¦
        total_ops = 200  # 100æ¬¡è®¾ç½® + 100æ¬¡è·å–
        total_time = set_time + get_time
        ops_per_second = total_ops / total_time if total_time > 0 else 0
        
        print(f"âœ… æ“ä½œé€Ÿåº¦: {ops_per_second:.1f} æ“ä½œ/ç§’")
        
        # 13. æµ‹è¯•ç»Ÿè®¡ä¿¡æ¯
        print("\nğŸ“Š æµ‹è¯•ç»Ÿè®¡ä¿¡æ¯ï¼š")
        
        stats = redis_client.get_cache_stats()
        print("âœ… ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯:")
        print(f"   ç¼“å­˜å‘½ä¸­: {stats['cache_hits']}")
        print(f"   ç¼“å­˜æœªå‘½ä¸­: {stats['cache_misses']}")
        print(f"   å‘½ä¸­ç‡: {stats['hit_rate']:.2%}")
        print(f"   ç¼“å­˜è®¾ç½®: {stats['cache_sets']}")
        print(f"   ç¼“å­˜åˆ é™¤: {stats['cache_deletes']}")
        print(f"   å†…å­˜ä½¿ç”¨: {stats['memory_usage']}/{stats['memory_max_size']}")
        print(f"   Redisæ“ä½œ: {stats['redis_operations']}")
        print(f"   æ€»é”®æ•°: {stats['total_keys']}")
        print(f"   é”è·å–: {stats['lock_acquisitions']}")
        print(f"   é”å¤±è´¥: {stats['lock_failures']}")
        print(f"   é”æˆåŠŸç‡: {stats['lock_success_rate']:.2%}")
        print(f"   æ´»è·ƒé”: {stats['active_locks']}")
        
        # 14. æµ‹è¯•è¿æ¥ä¿¡æ¯
        print("\nğŸ“Š æµ‹è¯•è¿æ¥ä¿¡æ¯ï¼š")
        
        conn_info = redis_client.get_connection_info()
        print("âœ… è¿æ¥ä¿¡æ¯:")
        print(f"   è¿æ¥çŠ¶æ€: {conn_info['connected']}")
        print(f"   è¿æ¥æ± å¤§å°: {conn_info['connection_pool_size']}")
        print(f"   Redisç‰ˆæœ¬: {conn_info['redis_version']}")
        print(f"   å†…å­˜ä½¿ç”¨: {conn_info['memory_usage_mb']:.2f} MB")
        print(f"   è¿è¡Œæ—¶é—´: {conn_info['uptime_seconds']} ç§’")
        print(f"   æ€»å‘½ä»¤æ•°: {conn_info['total_commands_processed']}")
        
        # 15. æµ‹è¯•Ping
        print("\nğŸ“Š æµ‹è¯•Pingï¼š")
        
        ping_result = redis_client.ping()
        print(f"âœ… Pingç»“æœ: {ping_result}")
        
        # 16. æµ‹è¯•çŠ¶æ€æŸ¥è¯¢
        print("\nğŸ“Š æµ‹è¯•çŠ¶æ€æŸ¥è¯¢ï¼š")
        
        status = redis_client.get_status()
        print("âœ… å®¢æˆ·ç«¯çŠ¶æ€:")
        print(f"   è¿æ¥çŠ¶æ€: {status['connected']}")
        print(f"   æ€»æ“ä½œæ•°: {status['total_operations']}")
        print(f"   å†…å­˜ç¼“å­˜å¤§å°: {status['memory_cache_size']}")
        print(f"   Rediså­˜å‚¨å¤§å°: {status['redis_storage_size']}")
        print(f"   æ´»è·ƒé”æ•°: {status['active_locks']}")
        print(f"   ç¼“å­˜å‘½ä¸­ç‡: {status['cache_hit_rate']:.2%}")
        print(f"   è¿è¡ŒçŠ¶æ€: {status['is_operational']}")
        
        # 17. æµ‹è¯•é”™è¯¯å¤„ç†
        print("\nğŸ“Š æµ‹è¯•é”™è¯¯å¤„ç†ï¼š")
        
        # æµ‹è¯•è·å–ä¸å­˜åœ¨çš„é”®
        non_existent = redis_client.get("non_existent_key")
        print(f"âœ… è·å–ä¸å­˜åœ¨çš„é”®: {non_existent}")
        
        # æµ‹è¯•åˆ é™¤ä¸å­˜åœ¨çš„é”®
        delete_non_existent = redis_client.delete("non_existent_key")
        print(f"âœ… åˆ é™¤ä¸å­˜åœ¨çš„é”®: {delete_non_existent}")
        
        # æµ‹è¯•é‡Šæ”¾ä¸å­˜åœ¨çš„é”
        release_non_existent = redis_client.release_lock("non_existent_lock")
        print(f"âœ… é‡Šæ”¾ä¸å­˜åœ¨çš„é”: {release_non_existent}")
        
        # 18. æµ‹è¯•æ¸…ç©ºæ“ä½œ
        print("\nğŸ“Š æµ‹è¯•æ¸…ç©ºæ“ä½œï¼š")
        
        # æ·»åŠ ä¸€äº›æµ‹è¯•æ•°æ®
        redis_client.set("clear_test_1", "value1")
        redis_client.set("clear_test_2", "value2")
        redis_client.set_hash("clear_hash", "field", "value")
        
        # è·å–æ¸…ç©ºå‰çš„çŠ¶æ€
        before_stats = redis_client.get_cache_stats()
        print(f"âœ… æ¸…ç©ºå‰æ€»é”®æ•°: {before_stats['total_keys']}")
        
        # æ¸…ç©ºæ‰€æœ‰ç¼“å­˜
        flush_result = redis_client.flush_all()
        print(f"âœ… æ¸…ç©ºæ‰€æœ‰ç¼“å­˜: {flush_result}")
        
        # è·å–æ¸…ç©ºåçš„çŠ¶æ€
        after_stats = redis_client.get_cache_stats()
        print(f"âœ… æ¸…ç©ºåæ€»é”®æ•°: {after_stats['total_keys']}")
        
        # éªŒè¯æ¸…ç©ºæ•ˆæœ
        test_value = redis_client.get("clear_test_1")
        print(f"âœ… æ¸…ç©ºåè·å–æµ‹è¯•å€¼: {test_value}")
        
        print("\nâœ… Rediså®¢æˆ·ç«¯æµ‹è¯•å®Œæˆ")
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = test_redis_client()
    sys.exit(0 if success else 1) 