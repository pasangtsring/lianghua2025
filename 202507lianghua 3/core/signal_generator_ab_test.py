"""
ğŸ§ª ä»»åŠ¡5.2: A/Bæµ‹è¯•æ¡†æ¶ - ä¿¡å·ç”Ÿæˆå™¨æ€§èƒ½å¯¹æ¯”
å¯¹æ¯”åŸç‰ˆä¿¡å·ç”Ÿæˆå™¨ vs å¢å¼ºç‰ˆç»ˆæä¿¡å·ç”Ÿæˆå™¨

æ ¸å¿ƒåŠŸèƒ½ï¼š
1. å¹¶è¡Œè¿è¡Œä¸¤å¥—ä¿¡å·ç”Ÿæˆç³»ç»Ÿ
2. å®æ—¶æ”¶é›†æ€§èƒ½æŒ‡æ ‡
3. ç»Ÿè®¡åˆ†æå’Œå¯¹æ¯”æŠ¥å‘Š
4. è‡ªåŠ¨åŒ–å†³ç­–æ”¯æŒç³»ç»Ÿ
5. é£é™©æ§åˆ¶å’Œå›é€€æœºåˆ¶
"""

import asyncio
import numpy as np
import pandas as pd
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum
import logging
import json
import statistics

from utils.logger import get_logger, performance_monitor
from config.config_manager import ConfigManager
from core.signal_generator import SignalGeneratorWithEnhancedFilter, TradingSignal
from core.ultimate_multi_timeframe_signal_generator import UltimateMultiTimeframeSignalGenerator, EnhancedTradingSignal

class TestGroup(Enum):
    """æµ‹è¯•ç»„åˆ«æšä¸¾"""
    GROUP_A = "group_a"  # åŸç‰ˆä¿¡å·ç”Ÿæˆå™¨
    GROUP_B = "group_b"  # å¢å¼ºç‰ˆç»ˆæä¿¡å·ç”Ÿæˆå™¨

class TestPhase(Enum):
    """æµ‹è¯•é˜¶æ®µæšä¸¾"""
    INITIALIZATION = "initialization"    # åˆå§‹åŒ–
    RUNNING = "running"                 # è¿è¡Œä¸­
    ANALYSIS = "analysis"               # åˆ†æé˜¶æ®µ
    DECISION = "decision"               # å†³ç­–é˜¶æ®µ
    COMPLETED = "completed"             # å·²å®Œæˆ

@dataclass
class SignalPerformanceMetrics:
    """ä¿¡å·æ€§èƒ½æŒ‡æ ‡æ•°æ®ç»“æ„"""
    # åŸºç¡€ç»Ÿè®¡
    total_signals: int = 0
    valid_signals: int = 0
    
    # ä¿¡å·è´¨é‡
    avg_confidence: float = 0.0
    max_confidence: float = 0.0
    confidence_distribution: Dict[str, int] = field(default_factory=dict)
    
    # ä¿¡å·ç±»å‹åˆ†å¸ƒ
    buy_signals: int = 0
    sell_signals: int = 0
    hold_signals: int = 0
    
    # ä¿¡å·å¼ºåº¦åˆ†å¸ƒ
    strength_distribution: Dict[str, int] = field(default_factory=dict)
    
    # æ—¶é—´ç»Ÿè®¡
    avg_generation_time: float = 0.0
    total_generation_time: float = 0.0
    
    # å½¢æ€å¢å¼ºç»Ÿè®¡ï¼ˆä»…Bç»„ï¼‰
    pattern_enhanced_signals: int = 0
    double_pattern_found: int = 0
    
    def calculate_derived_metrics(self):
        """è®¡ç®—è¡ç”ŸæŒ‡æ ‡"""
        self.signal_success_rate = self.valid_signals / max(self.total_signals, 1)
        self.pattern_enhancement_rate = self.pattern_enhanced_signals / max(self.valid_signals, 1)
        self.double_pattern_rate = self.double_pattern_found / max(self.valid_signals, 1)

@dataclass
class ABTestConfig:
    """A/Bæµ‹è¯•é…ç½®"""
    # æµ‹è¯•æŒç»­æ—¶é—´
    test_duration_days: int = 30
    test_duration_hours: int = 24 * 30  # 30å¤© = 720å°æ—¶ 
    
    # æµé‡åˆ†é…
    traffic_split: float = 0.5  # 50%æµé‡ç»™Bç»„
    
    # æˆåŠŸæŒ‡æ ‡
    success_metrics: List[str] = field(default_factory=lambda: [
        'signal_success_rate', 'avg_confidence', 'pattern_enhancement_rate'
    ])
    
    # ç»Ÿè®¡æ˜¾è‘—æ€§
    min_sample_size: int = 100      # æœ€å°æ ·æœ¬é‡
    significance_level: float = 0.05 # æ˜¾è‘—æ€§æ°´å¹³
    
    # é£é™©æ§åˆ¶
    max_performance_degradation: float = 0.1  # æœ€å¤§æ€§èƒ½ä¸‹é™10%
    emergency_stop_threshold: float = 0.2     # ç´§æ€¥åœæ­¢é˜ˆå€¼20%
    
    # æ•°æ®æ”¶é›†
    metrics_collection_interval: int = 3600   # æ¯å°æ—¶æ”¶é›†ä¸€æ¬¡æŒ‡æ ‡
    detailed_logging: bool = True             # è¯¦ç»†æ—¥å¿—
    
    # å†³ç­–æ ‡å‡†
    improvement_threshold: float = 0.05       # æ”¹è¿›é˜ˆå€¼5%
    confidence_threshold: float = 0.95        # ç½®ä¿¡åº¦é˜ˆå€¼95%

class SignalGeneratorABTest:
    """
    ğŸ§ª ä¿¡å·ç”Ÿæˆå™¨A/Bæµ‹è¯•ç®¡ç†å™¨
    
    æµ‹è¯•è®¾è®¡ï¼š
    - Aç»„ï¼šåŸç‰ˆSignalGeneratorWithEnhancedFilter
    - Bç»„ï¼šå¢å¼ºç‰ˆUltimateMultiTimeframeSignalGenerator
    - è¯„ä¼°æŒ‡æ ‡ï¼šä¿¡å·è´¨é‡ã€ç”Ÿæˆé€Ÿåº¦ã€å¢å¼ºæ•ˆæœ
    """
    
    def __init__(self, config: ConfigManager, test_config: Optional[ABTestConfig] = None):
        self.config = config
        self.test_config = test_config or ABTestConfig()
        self.logger = get_logger(__name__)
        
        # åˆå§‹åŒ–ä¿¡å·ç”Ÿæˆå™¨
        self.group_a_generator = SignalGeneratorWithEnhancedFilter(config)
        self.group_b_generator = UltimateMultiTimeframeSignalGenerator(config)
        
        # æµ‹è¯•çŠ¶æ€
        self.current_phase = TestPhase.INITIALIZATION
        self.test_start_time = None
        self.test_end_time = None
        
        # æ€§èƒ½æŒ‡æ ‡æ”¶é›†
        self.group_a_metrics = SignalPerformanceMetrics()
        self.group_b_metrics = SignalPerformanceMetrics()
        
        # è¯¦ç»†æ•°æ®è®°å½•
        self.detailed_results = {
            'group_a': [],
            'group_b': []
        }
        
        # å®æ—¶ç»Ÿè®¡
        self.hourly_metrics = {}
        self.comparison_results = {}
        
        self.logger.info("ğŸ§ª A/Bæµ‹è¯•æ¡†æ¶åˆå§‹åŒ–å®Œæˆ")
        self.logger.info(f"   ğŸ“Š æµ‹è¯•é…ç½®: {self.test_config.test_duration_days}å¤©, æµé‡åˆ†é…={self.test_config.traffic_split}")
        self.logger.info(f"   ğŸ¯ æˆåŠŸæŒ‡æ ‡: {self.test_config.success_metrics}")
    
    async def start_ab_test(self) -> Dict[str, Any]:
        """
        ğŸš€ å¯åŠ¨A/Bæµ‹è¯•
        
        Returns:
            æµ‹è¯•å¯åŠ¨ç»“æœ
        """
        try:
            self.logger.info("ğŸš€ å¯åŠ¨A/Bæµ‹è¯•...")
            
            # 1. åˆå§‹åŒ–æµ‹è¯•ç¯å¢ƒ
            await self._initialize_test_environment()
            
            # 2. å¼€å§‹æµ‹è¯•
            self.current_phase = TestPhase.RUNNING
            self.test_start_time = datetime.now()
            self.test_end_time = self.test_start_time + timedelta(days=self.test_config.test_duration_days)
            
            self.logger.info(f"âœ… A/Bæµ‹è¯•å·²å¯åŠ¨")
            self.logger.info(f"   â° å¼€å§‹æ—¶é—´: {self.test_start_time}")
            self.logger.info(f"   â° ç»“æŸæ—¶é—´: {self.test_end_time}")
            
            return {
                'status': 'started',
                'start_time': self.test_start_time.isoformat(),
                'end_time': self.test_end_time.isoformat(),
                'phase': self.current_phase.value
            }
            
        except Exception as e:
            self.logger.error(f"âŒ A/Bæµ‹è¯•å¯åŠ¨å¤±è´¥: {e}")
            return {'status': 'failed', 'error': str(e)}
    
    async def _initialize_test_environment(self):
        """åˆå§‹åŒ–æµ‹è¯•ç¯å¢ƒ"""
        # éªŒè¯ä¸¤ä¸ªç”Ÿæˆå™¨éƒ½æ­£å¸¸å·¥ä½œ
        test_data = pd.DataFrame({
            'open': [100, 101, 99, 102],
            'high': [102, 103, 101, 104],
            'low': [99, 100, 98, 101],
            'close': [101, 99, 102, 103],
            'volume': [1000, 1200, 800, 1500]
        })
        
        # Aç»„æµ‹è¯•
        try:
            a_signal = self.group_a_generator.generate_signal(test_data.to_dict('records'))
            self.logger.info("âœ… Aç»„ä¿¡å·ç”Ÿæˆå™¨æµ‹è¯•é€šè¿‡")
        except Exception as e:
            self.logger.error(f"âŒ Aç»„ä¿¡å·ç”Ÿæˆå™¨æµ‹è¯•å¤±è´¥: {e}")
            raise
        
        # Bç»„æµ‹è¯• (éœ€è¦å¤šæ—¶é—´å‘¨æœŸæ•°æ®)
        try:
            multi_data = {
                'trend': test_data,
                'signal': test_data, 
                'entry': test_data
            }
            b_signal = await self.group_b_generator.generate_ultimate_signal('BTCUSDT', multi_data)
            self.logger.info("âœ… Bç»„ä¿¡å·ç”Ÿæˆå™¨æµ‹è¯•é€šè¿‡")
        except Exception as e:
            self.logger.error(f"âŒ Bç»„ä¿¡å·ç”Ÿæˆå™¨æµ‹è¯•å¤±è´¥: {e}")
            raise
    
    async def run_parallel_signal_generation(self, symbol: str, single_tf_data: List[Dict], multi_tf_data: Dict[str, pd.DataFrame]) -> Dict[str, Any]:
        """
        ğŸ”„ å¹¶è¡Œè¿è¡Œä¸¤å¥—ä¿¡å·ç”Ÿæˆç³»ç»Ÿ
        
        Args:
            symbol: äº¤æ˜“å¯¹ç¬¦å·
            single_tf_data: å•æ—¶é—´å‘¨æœŸæ•°æ®ï¼ˆAç»„ä½¿ç”¨ï¼‰
            multi_tf_data: å¤šæ—¶é—´å‘¨æœŸæ•°æ®ï¼ˆBç»„ä½¿ç”¨ï¼‰
            
        Returns:
            å¹¶è¡Œæµ‹è¯•ç»“æœ
        """
        try:
            # æ ¹æ®æµé‡åˆ†é…å†³å®šæ˜¯å¦è¿è¡ŒBç»„
            run_group_b = np.random.random() < self.test_config.traffic_split
            
            results = {
                'timestamp': datetime.now().isoformat(),
                'symbol': symbol,
                'group_a_result': None,
                'group_b_result': None,
                'comparison': None
            }
            
            # Aç»„ä¿¡å·ç”Ÿæˆ
            start_time_a = datetime.now()
            try:
                a_signal = self.group_a_generator.generate_signal(single_tf_data)
                generation_time_a = (datetime.now() - start_time_a).total_seconds()
                
                results['group_a_result'] = {
                    'signal': a_signal.to_dict() if a_signal else None,
                    'generation_time': generation_time_a,
                    'success': a_signal is not None
                }
                
                # æ›´æ–°Aç»„æŒ‡æ ‡
                await self._update_group_metrics(TestGroup.GROUP_A, a_signal, generation_time_a)
                
            except Exception as e:
                self.logger.error(f"Aç»„ä¿¡å·ç”Ÿæˆå¤±è´¥ {symbol}: {e}")
                results['group_a_result'] = {'error': str(e), 'success': False}
            
            # Bç»„ä¿¡å·ç”Ÿæˆï¼ˆæŒ‰æµé‡åˆ†é…ï¼‰
            if run_group_b:
                start_time_b = datetime.now()
                try:
                    b_signal = await self.group_b_generator.generate_ultimate_signal(symbol, multi_tf_data)
                    generation_time_b = (datetime.now() - start_time_b).total_seconds()
                    
                    results['group_b_result'] = {
                        'signal': b_signal.__dict__ if b_signal else None,
                        'generation_time': generation_time_b,
                        'success': b_signal is not None
                    }
                    
                    # æ›´æ–°Bç»„æŒ‡æ ‡
                    await self._update_group_metrics(TestGroup.GROUP_B, b_signal, generation_time_b)
                    
                except Exception as e:
                    self.logger.error(f"Bç»„ä¿¡å·ç”Ÿæˆå¤±è´¥ {symbol}: {e}")
                    results['group_b_result'] = {'error': str(e), 'success': False}
            
            # è®°å½•è¯¦ç»†ç»“æœ
            if results['group_a_result']:
                self.detailed_results['group_a'].append(results['group_a_result'])
            if results['group_b_result']:
                self.detailed_results['group_b'].append(results['group_b_result'])
            
            return results
            
        except Exception as e:
            self.logger.error(f"å¹¶è¡Œä¿¡å·ç”Ÿæˆå¤±è´¥ {symbol}: {e}")
            return {'error': str(e)}
    
    async def _update_group_metrics(self, group: TestGroup, signal: Any, generation_time: float):
        """æ›´æ–°ç»„åˆ«æ€§èƒ½æŒ‡æ ‡"""
        metrics = self.group_a_metrics if group == TestGroup.GROUP_A else self.group_b_metrics
        
        # åŸºç¡€ç»Ÿè®¡
        metrics.total_signals += 1
        metrics.total_generation_time += generation_time
        metrics.avg_generation_time = metrics.total_generation_time / metrics.total_signals
        
        if signal:
            metrics.valid_signals += 1
            
            # ä¿¡å·ç½®ä¿¡åº¦
            confidence = getattr(signal, 'confidence', 0.0)
            metrics.avg_confidence = (metrics.avg_confidence * (metrics.valid_signals - 1) + confidence) / metrics.valid_signals
            metrics.max_confidence = max(metrics.max_confidence, confidence)
            
            # ä¿¡å·ç±»å‹ç»Ÿè®¡
            signal_type = getattr(signal, 'signal_type', None)
            if signal_type:
                if signal_type.name == 'BUY':
                    metrics.buy_signals += 1
                elif signal_type.name == 'SELL':
                    metrics.sell_signals += 1
                else:
                    metrics.hold_signals += 1
            
            # ä¿¡å·å¼ºåº¦ç»Ÿè®¡
            signal_strength = getattr(signal, 'signal_strength', None)
            if signal_strength:
                strength_key = signal_strength.name if hasattr(signal_strength, 'name') else str(signal_strength)
                metrics.strength_distribution[strength_key] = metrics.strength_distribution.get(strength_key, 0) + 1
            
            # Bç»„ç‰¹æœ‰çš„å¢å¼ºç»Ÿè®¡
            if group == TestGroup.GROUP_B and hasattr(signal, 'pattern_confirmation'):
                if signal.pattern_confirmation:
                    metrics.pattern_enhanced_signals += 1
                    # ä¿®å¤NoneTypeé”™è¯¯ï¼šæ·»åŠ metadataç©ºå€¼æ£€æŸ¥
                    if hasattr(signal, 'metadata') and signal.metadata and signal.metadata.get('double_pattern_found', False):
                        metrics.double_pattern_found += 1
        
        # è®¡ç®—è¡ç”ŸæŒ‡æ ‡
        metrics.calculate_derived_metrics()
    
    def get_current_comparison(self) -> Dict[str, Any]:
        """
        ğŸ“Š è·å–å½“å‰å¯¹æ¯”ç»“æœ
        
        Returns:
            è¯¦ç»†çš„å¯¹æ¯”åˆ†æç»“æœ  
        """
        try:
            # åŸºç¡€æŒ‡æ ‡å¯¹æ¯”
            comparison = {
                'test_info': {
                    'phase': self.current_phase.value,
                    'start_time': self.test_start_time.isoformat() if self.test_start_time else None,
                    'runtime_hours': (datetime.now() - self.test_start_time).total_seconds() / 3600 if self.test_start_time else 0
                },
                'group_a_metrics': self._metrics_to_dict(self.group_a_metrics),
                'group_b_metrics': self._metrics_to_dict(self.group_b_metrics),
                'comparison_analysis': {}
            }
            
            # å¯¹æ¯”åˆ†æ
            a_metrics = self.group_a_metrics
            b_metrics = self.group_b_metrics
            
            if a_metrics.total_signals > 0 and b_metrics.total_signals > 0:
                comparison['comparison_analysis'] = {
                    'signal_generation': {
                        'total_signals_ratio': b_metrics.total_signals / a_metrics.total_signals,
                        'valid_signals_ratio': b_metrics.valid_signals / max(a_metrics.valid_signals, 1),
                        'success_rate_diff': b_metrics.signal_success_rate - a_metrics.signal_success_rate
                    },
                    'quality_metrics': {
                        'confidence_improvement': b_metrics.avg_confidence - a_metrics.avg_confidence,
                        'max_confidence_diff': b_metrics.max_confidence - a_metrics.max_confidence
                    },
                    'performance_metrics': {
                        'speed_ratio': a_metrics.avg_generation_time / max(b_metrics.avg_generation_time, 0.001),
                        'pattern_enhancement_rate': b_metrics.pattern_enhancement_rate,
                        'double_pattern_rate': b_metrics.double_pattern_rate
                    }
                }
                
                # ç»Ÿè®¡æ˜¾è‘—æ€§åˆ†æï¼ˆç®€åŒ–ç‰ˆï¼‰
                comparison['statistical_analysis'] = self._perform_statistical_analysis()
            
            return comparison
            
        except Exception as e:
            self.logger.error(f"è·å–å¯¹æ¯”ç»“æœå¤±è´¥: {e}")
            return {'error': str(e)}
    
    def _metrics_to_dict(self, metrics: SignalPerformanceMetrics) -> Dict[str, Any]:
        """å°†æŒ‡æ ‡å¯¹è±¡è½¬æ¢ä¸ºå­—å…¸"""
        return {
            'total_signals': metrics.total_signals,
            'valid_signals': metrics.valid_signals,
            'signal_success_rate': metrics.signal_success_rate,
            'avg_confidence': metrics.avg_confidence,
            'max_confidence': metrics.max_confidence,
            'avg_generation_time': metrics.avg_generation_time,
            'buy_signals': metrics.buy_signals,
            'sell_signals': metrics.sell_signals,
            'strength_distribution': metrics.strength_distribution,
            'pattern_enhanced_signals': metrics.pattern_enhanced_signals,
            'double_pattern_found': metrics.double_pattern_found,
            'pattern_enhancement_rate': metrics.pattern_enhancement_rate,
            'double_pattern_rate': metrics.double_pattern_rate
        }
    
    def _perform_statistical_analysis(self) -> Dict[str, Any]:
        """æ‰§è¡Œç»Ÿè®¡æ˜¾è‘—æ€§åˆ†æï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        a_success_rate = self.group_a_metrics.signal_success_rate
        b_success_rate = self.group_b_metrics.signal_success_rate
        
        # æ ·æœ¬é‡æ£€æŸ¥
        min_sample_met = (self.group_a_metrics.total_signals >= self.test_config.min_sample_size and 
                         self.group_b_metrics.total_signals >= self.test_config.min_sample_size)
        
        # æ”¹è¿›ç¨‹åº¦
        improvement = b_success_rate - a_success_rate
        relative_improvement = improvement / max(a_success_rate, 0.001)
        
        # æ˜¯å¦æ˜¾è‘—æ”¹è¿›
        significant_improvement = (abs(improvement) > self.test_config.improvement_threshold and 
                                 min_sample_met)
        
        return {
            'min_sample_size_met': min_sample_met,
            'absolute_improvement': improvement,
            'relative_improvement': relative_improvement,
            'significant_improvement': significant_improvement,
            'recommendation': self._generate_recommendation(improvement, significant_improvement, min_sample_met)
        }
    
    def _generate_recommendation(self, improvement: float, significant: bool, sufficient_sample: bool) -> str:
        """ç”Ÿæˆæµ‹è¯•å»ºè®®"""
        if not sufficient_sample:
            return "æ ·æœ¬é‡ä¸è¶³ï¼Œéœ€è¦ç»§ç»­æµ‹è¯•"
        
        if significant and improvement > 0:
            return "Bç»„æ˜¾è‘—ä¼˜äºAç»„ï¼Œå»ºè®®é‡‡ç”¨å¢å¼ºç‰ˆä¿¡å·ç”Ÿæˆå™¨"
        elif significant and improvement < 0:
            return "Aç»„æ˜¾è‘—ä¼˜äºBç»„ï¼Œå»ºè®®ä¿æŒåŸç‰ˆä¿¡å·ç”Ÿæˆå™¨"
        else:
            return "ä¸¤ç»„å·®å¼‚ä¸æ˜¾è‘—ï¼Œå»ºè®®å»¶é•¿æµ‹è¯•æ—¶é—´æˆ–ä¿æŒç°çŠ¶"
    
    async def check_emergency_stop_conditions(self) -> bool:
        """æ£€æŸ¥ç´§æ€¥åœæ­¢æ¡ä»¶"""
        if self.group_b_metrics.total_signals < 10:  # æ ·æœ¬å¤ªå°‘
            return False
        
        # æ£€æŸ¥Bç»„æ€§èƒ½æ˜¯å¦ä¸¥é‡ä¸‹é™
        a_success_rate = self.group_a_metrics.signal_success_rate
        b_success_rate = self.group_b_metrics.signal_success_rate
        
        if a_success_rate > 0:
            performance_degradation = (a_success_rate - b_success_rate) / a_success_rate
            if performance_degradation > self.test_config.emergency_stop_threshold:
                self.logger.warning(f"âš ï¸ æ£€æµ‹åˆ°ä¸¥é‡æ€§èƒ½ä¸‹é™: {performance_degradation:.2%}")
                return True
        
        return False
    
    def export_test_results(self, filepath: str) -> bool:
        """
        ğŸ“¤ å¯¼å‡ºæµ‹è¯•ç»“æœ
        
        Args:
            filepath: å¯¼å‡ºæ–‡ä»¶è·¯å¾„
            
        Returns:
            å¯¼å‡ºæ˜¯å¦æˆåŠŸ
        """
        try:
            results = {
                'test_config': {
                    'duration_days': self.test_config.test_duration_days,
                    'traffic_split': self.test_config.traffic_split,
                    'success_metrics': self.test_config.success_metrics
                },
                'test_summary': {
                    'start_time': self.test_start_time.isoformat() if self.test_start_time else None,
                    'end_time': self.test_end_time.isoformat() if self.test_end_time else None,
                    'current_phase': self.current_phase.value
                },
                'results': self.get_current_comparison(),
                'detailed_data': {
                    'group_a_signals': len(self.detailed_results['group_a']),
                    'group_b_signals': len(self.detailed_results['group_b'])
                }
            }
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, ensure_ascii=False)
            
            self.logger.info(f"âœ… æµ‹è¯•ç»“æœå·²å¯¼å‡º: {filepath}")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ å¯¼å‡ºæµ‹è¯•ç»“æœå¤±è´¥: {e}")
            return False
    
    def get_test_status(self) -> Dict[str, Any]:
        """è·å–æµ‹è¯•çŠ¶æ€æ‘˜è¦"""
        return {
            'phase': self.current_phase.value,
            'runtime_info': {
                'start_time': self.test_start_time.isoformat() if self.test_start_time else None,
                'runtime_hours': (datetime.now() - self.test_start_time).total_seconds() / 3600 if self.test_start_time else 0,
                'remaining_hours': (self.test_end_time - datetime.now()).total_seconds() / 3600 if self.test_end_time else 0
            },
            'sample_sizes': {
                'group_a': self.group_a_metrics.total_signals,
                'group_b': self.group_b_metrics.total_signals
            },
            'quick_comparison': {
                'group_a_success_rate': self.group_a_metrics.signal_success_rate,
                'group_b_success_rate': self.group_b_metrics.signal_success_rate,
                'group_b_pattern_rate': self.group_b_metrics.pattern_enhancement_rate
            }
        } 